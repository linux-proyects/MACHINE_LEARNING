{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ba6ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'NAIVE_BAYES/.ipynb_checkpoints/naive_bayes_scratch-checkpoint.ipynb', LF will be replaced by CRLF the next time Git touches it\n",
      "warning: in the working copy of 'NAIVE_BAYES/naive_bayes_scratch.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7a92aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f8cac07] naive_bayes_1\n",
      " 2 files changed, 12 insertions(+)\n",
      " create mode 100644 NAIVE_BAYES/.ipynb_checkpoints/naive_bayes_scratch-checkpoint.ipynb\n",
      " create mode 100644 NAIVE_BAYES/naive_bayes_scratch.ipynb\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"naive_bayes_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9034c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/linux-proyects/MACHINE_LEARNING.git\n",
      "   47c4bd4..f8cac07  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4828f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544b4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    #We don't add the init function because we don't have any parameters \n",
    "    def fit(self,x,y):\n",
    "        # get the number of samples and cfeatures \n",
    "        n_samples,n_features=x.shape\n",
    "        # obtain unique classes \n",
    "        self._classes=np.unique(y)\n",
    "        # obtain number of classes \n",
    "        n_classes=len(self._classes)\n",
    "        \n",
    "        # Calculate mean, var and prob apriori for each class\n",
    "            # Initialize the arrays\n",
    "        self._mean= np.zeros((n_classes,n_features), dtype=np.float64)\n",
    "        self._var= np.zeros((n_classes,n_features), dtype=np.float64)\n",
    "        self._prior= np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        for idx , c  in enumerate(self._classes):\n",
    "            # Obtain the examples for class c \n",
    "            X_c = x[y==c]\n",
    "            self._mean[idx,:]= X_c.mean(axis=0) \n",
    "            self._var[idx,:]= X_c.var(axis=0)\n",
    "            self._prior= X_c.shape[0] / float(n_samples)\n",
    "\n",
    "\n",
    "\n",
    "#      def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         self._classes = np.unique(y)\n",
    "#         n_classes = len(self._classes)\n",
    "\n",
    "#         # calculate mean, var, and prior for each class\n",
    "#         self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "#         self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "#         self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "#         for idx, c in enumerate(self._classes):\n",
    "#             X_c = X[y == c]\n",
    "#             self._mean[idx, :] = X_c.mean(axis=0)\n",
    "#             self._var[idx, :] = X_c.var(axis=0)\n",
    "#             self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)       \n",
    "            \n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # return class with the highest posterior\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "            \n",
    "        \n",
    "#     def predict(self,X):\n",
    "        \n",
    "#         y_pred=[self._predict(x) for x in X]\n",
    "#         return np.array(y_pred)\n",
    "    \n",
    "#     def _predict(self,x):\n",
    "#         # save the posterior prob for each class c\n",
    "#         posteriors=[]\n",
    "        \n",
    "#         for idx, c in enumerate(self.classes):\n",
    "#             prior_i= np.log(self.prior[idx])\n",
    "#             # the sum of the posterior prob for each x_i and prior_y_i\n",
    "#             posterior= np.sum(np.log(self._posterior_prob(idx,x))) # we pass xi,and the idx for the class c\n",
    "#             posterior= posterior + prior\n",
    "#             # append the posterior probability for y_i or class c\n",
    "#             posteriors.append(posterior)\n",
    "            \n",
    "#         # return the class with the height posterior probability\n",
    "#         return self.classes[np.argmax(posteriors)]\n",
    "            \n",
    "#     def _posterior_prob(self,idx,x):\n",
    "\n",
    "#         # obtain mean,var,and aprior for class c\n",
    "#         mean,var,prior=self.mean[idx,:],self.var[idx,:],self.prior[idx]\n",
    "#         # aply the gausian formula to obtain the posterior prob for xi and y_i (class_i)\n",
    "#             # this is the sum of the posterior probability \n",
    "#         numerator=np.exp(-(x-mean)**2 / (2*var))\n",
    "#         denominator=np.sqrt(2*np.pi*var)\n",
    "#         return numerator / denominator \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2178277",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m nb \u001b[38;5;241m=\u001b[39m NaiveBayes()\n\u001b[0;32m     19\u001b[0m nb\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive Bayes classification accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy(y_test, predictions))\n",
      "Cell \u001b[1;32mIn[34], line 43\u001b[0m, in \u001b[0;36mNaiveBayes.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 43\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(y_pred)\n",
      "Cell \u001b[1;32mIn[34], line 43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 43\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(y_pred)\n",
      "Cell \u001b[1;32mIn[34], line 51\u001b[0m, in \u001b[0;36mNaiveBayes._predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# calculate posterior probability for each class\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classes):\n\u001b[1;32m---> 51\u001b[0m     prior \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior[idx])\n\u001b[0;32m     52\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pdf(idx, x)))\n\u001b[0;32m     53\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m posterior \u001b[38;5;241m+\u001b[39m prior\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import datasets\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    X, y = datasets.make_classification(\n",
    "        n_samples=1000, n_features=10, n_classes=2, random_state=123\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=123\n",
    "    )\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(X_train, y_train)\n",
    "    predictions = nb.predict(X_test)\n",
    "\n",
    "    print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f441f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7d603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2e600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6b1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb5a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean , var and probability a priory(frequencie) for each class \n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data,label=make_blobs(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    centers=2,\n",
    "    cluster_std=1.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a06013cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "[[-3.08420671  8.79149536]\n",
      " [-3.16245689  9.14739324]\n",
      " [-2.0755229  10.85993001]\n",
      " [-2.97972741  7.25806819]\n",
      " [-3.51625865  8.64323128]\n",
      " [-2.42816378  7.3045883 ]\n",
      " [-3.07256689  9.66535818]\n",
      " [-4.10302088  9.25051961]\n",
      " [-2.39908469  6.62920343]\n",
      " [-0.61414224  9.9352078 ]\n",
      " [-1.53416664 10.64177416]\n",
      " [-2.79018167  8.73332178]\n",
      " [-4.28342401  8.15047308]\n",
      " [-3.89038992  9.46512375]\n",
      " [-0.7504191   8.74335457]\n",
      " [-2.09685568  6.89863794]\n",
      " [-2.11168151 10.18494028]\n",
      " [-3.47938995  8.41217768]\n",
      " [-2.25856131  6.66268198]\n",
      " [-1.62303773  9.21992807]\n",
      " [-0.73572477  8.39236187]\n",
      " [-2.55218887 10.89185852]\n",
      " [-2.15385329  9.32755245]\n",
      " [-3.83679959  7.57883818]\n",
      " [-4.57909902  8.33954109]\n",
      " [-3.06529885  8.45541042]\n",
      " [-2.2188429   6.71834983]\n",
      " [-2.39270456 10.17666012]\n",
      " [-2.07523439  8.24014242]\n",
      " [-2.40474114  8.65547731]\n",
      " [-3.14490987  9.63020705]\n",
      " [-4.20764251  8.50951174]\n",
      " [-3.06196415 10.2828328 ]\n",
      " [-2.59560977 10.21852561]\n",
      " [-2.52539429  7.74503301]\n",
      " [-3.23124556 11.23701995]\n",
      " [-1.52214373  7.54927375]\n",
      " [-3.59882651  7.31952169]\n",
      " [-2.64797556  8.65296169]\n",
      " [-3.35166134  8.62109155]\n",
      " [-5.65289175 10.00056913]\n",
      " [-3.72459497  9.39138293]\n",
      " [-3.22996405  8.66425363]\n",
      " [-2.12029686  8.55218739]\n",
      " [-3.32150402  9.74829767]\n",
      " [-2.92045504  8.05155341]\n",
      " [-1.2719982  10.13182227]\n",
      " [-1.41071508  9.40878746]\n",
      " [-2.77280389  9.44282121]\n",
      " [-2.50306148  8.73278157]]\n",
      "hola\n",
      "[-2.74166809  8.86528073]\n",
      "1 1\n",
      "[[5.93954033 3.23773215]\n",
      " [5.62635103 4.24932126]\n",
      " [4.91283076 3.54174099]\n",
      " [4.9719078  2.96578958]\n",
      " [2.77908272 2.05544525]\n",
      " [3.15553798 0.38862175]\n",
      " [3.82384917 2.25187412]\n",
      " [3.69997489 1.58669586]\n",
      " [4.71215509 4.92906022]\n",
      " [3.35480784 2.55213658]\n",
      " [4.7097293  0.60160533]\n",
      " [4.89679133 0.47828315]\n",
      " [4.40904568 2.33502649]\n",
      " [5.58911717 0.88190474]\n",
      " [5.79593019 2.4685068 ]\n",
      " [4.95173819 2.91135713]\n",
      " [4.54735678 2.38255205]\n",
      " [5.26620871 2.32955129]\n",
      " [4.32229064 5.23737268]\n",
      " [5.34410735 4.60171643]\n",
      " [4.84769595 2.43555054]\n",
      " [5.6160995  0.4961325 ]\n",
      " [3.47026083 2.91767121]\n",
      " [5.20776399 1.88577499]\n",
      " [4.3454131  1.06868629]\n",
      " [3.21031464 2.76103401]\n",
      " [4.65548111 3.71741058]\n",
      " [5.06922367 2.64611112]\n",
      " [5.39067965 0.94458062]\n",
      " [4.94893931 1.88383459]\n",
      " [3.57246152 0.99419734]\n",
      " [4.37172349 2.82997028]\n",
      " [2.33735338 1.94135303]\n",
      " [6.03019353 0.9883509 ]\n",
      " [3.57924991 2.15763981]\n",
      " [6.90330192 2.18266306]\n",
      " [2.71089895 2.19473031]\n",
      " [4.94047026 2.38890754]\n",
      " [3.45123525 1.29361241]\n",
      " [6.87880825 2.54176919]\n",
      " [6.01126621 2.87548932]\n",
      " [4.10406089 3.00084824]\n",
      " [4.44633598 2.45803071]\n",
      " [4.75946047 1.3689989 ]\n",
      " [3.53656976 3.83309097]\n",
      " [4.9915658  1.11594798]\n",
      " [3.36511438 2.5414806 ]\n",
      " [4.59822471 0.57075604]\n",
      " [6.32323201 0.29094841]\n",
      " [3.62372637 0.15535301]]\n",
      "hola\n",
      "[4.60210955 2.20934437]\n"
     ]
    }
   ],
   "source": [
    "# Calculate posterior for each class\n",
    "classes=np.unique(label)\n",
    "\n",
    "\n",
    "for idx,c in enumerate(classes):\n",
    "    print(idx,c)\n",
    "    #fancy index\n",
    "    X_c=data[label==c]\n",
    "    print(X_c)\n",
    "    print(\"hola\")\n",
    "    print(X_c.mean(axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose class with highest posterior probability \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
